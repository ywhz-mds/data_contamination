{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371daea8-3dd4-40e2-9679-a98dbccfc884",
   "metadata": {},
   "source": [
    "## 1.截断\n",
    "对于每一条的text信息，将其按照500词及以内为一段进行截取，要保证截取时上一句正好完整结束，  \n",
    "对于初始长度不足500的text记录，直接保留其text和label存入新json文件，  \n",
    "对于长度大于500的text记录，舍弃掉最后长度不足200词的尾段，且label保持与截取前的相同。  \n",
    "示例如下：一个label为clean的长度为2100词的text，其处理为4个长度在500词左右的label为clean的text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e951c343-1605-476a-9608-1659adba3604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut into 500 success!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def process_texts(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    processed_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        text = entry['text']\n",
    "        label = entry['label']\n",
    "        words = text.split()\n",
    "        num_words = len(words)\n",
    "\n",
    "        if num_words < 500:\n",
    "            # 如果文本长度不足500词，直接添加到处理后的数据中\n",
    "            processed_data.append({'text': text, 'label': label})\n",
    "        else:\n",
    "            # 按500词分段\n",
    "            i = 0\n",
    "            while i < num_words:\n",
    "                next_limit = i + 500 if i + 500 < num_words else num_words\n",
    "                next_chunk = ' '.join(words[i:next_limit])\n",
    "                \n",
    "                if next_limit - i < 500:\n",
    "                    if next_limit - i < 200:\n",
    "                        break  # 如果剩余词数小于200，则丢弃\n",
    "                    else:\n",
    "                        # 如果剩余词数在200到500之间，则保留\n",
    "                        processed_data.append({'text': next_chunk, 'label': label})\n",
    "                        break\n",
    "                \n",
    "                # 寻找500词之后的句子结束位置\n",
    "                end_index = next_chunk.rfind('.')\n",
    "                if end_index == -1:\n",
    "                    end_index = next_chunk.rfind('!')\n",
    "                if end_index == -1:\n",
    "                    end_index = next_chunk.rfind('?')\n",
    "                \n",
    "                if end_index != -1:\n",
    "                    # 确保只包括完整的句子\n",
    "                    next_chunk = next_chunk[:end_index+1]\n",
    "                    processed_data.append({'text': next_chunk, 'label': label})\n",
    "                \n",
    "                # 更新索引位置，继续下一个500词段\n",
    "                i += end_index + 1 if end_index != -1 else 500\n",
    "\n",
    "    # 将处理后的数据保存到新的JSON文件中\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(processed_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 使用该函数处理数据\n",
    "process_texts('dataset/valid.json', 'dataset/preprocessed_valid.json')\n",
    "\n",
    "print(\"cut into 500 success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4d7be-c0bf-491f-b00a-11362cba4f3b",
   "metadata": {},
   "source": [
    "## 2.处理补全+打分\n",
    "对于上述程序生成的经过预处理的原文件，对每条text，将其按照某比例ratio进行截断，需保证截断处是完整句子的结束，将截断处之前的部分截取出来，作为text用detect model进行补全，补全至500词左右形成新的text。对于新的text，其label不再是clean或dirty，而是一个介于0与1之间的value值，value值生成规则为：原有text的label若为clean则ori_val=0，若为dirty则ori_val=1，新生成的值val=ori_val*ratio + const*(1-ratio)*softmax() ，其中const是一个人为设定的常数0.1，ratio是截断比例，softmax是非线性的激发函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff14b68e-5f7c-46ea-bb1b-f72cd5525220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build data success!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 载入经过预处理的数据\n",
    "with open('dataset/valid.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 设置detect model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"detected_model\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"detected_model\", torch_dtype=torch.bfloat16)\n",
    "# text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=500)\n",
    "\n",
    "# 定义softmax函数\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# 定义常数和比例\n",
    "const = 0.1\n",
    "ratio = 0.25  # 可以根据需要调整比例\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for entry in data:\n",
    "    text = entry['text']\n",
    "    label = entry['label']\n",
    "\n",
    "    # 根据label设置原始value\n",
    "    ori_val = 0 if label == 'clean' else 1\n",
    "\n",
    "    # 按照比例截断\n",
    "    words = text.split()\n",
    "    cut_index = int(len(words) * ratio)\n",
    "    truncated_text = ' '.join(words[:cut_index])\n",
    "\n",
    "    # 确保在完整的句子结束处截断\n",
    "    end_punctuation = max(truncated_text.rfind('.'), truncated_text.rfind('!'), truncated_text.rfind('?'))\n",
    "    if end_punctuation != -1:\n",
    "        truncated_text = truncated_text[:end_punctuation+1]\n",
    "\n",
    "    # 使用模型补全文本\n",
    "    # generated_text = text_generator(truncated_text, max_length=500)[0]['generated_text']\n",
    "    generated_text = truncated_text\n",
    "    \n",
    "    # 计算新的value\n",
    "    new_val = ori_val * ratio + const * (1 - ratio) * softmax(np.array([1]))[0]  # 简化了softmax的使用\n",
    "\n",
    "    # 添加到新的数据集\n",
    "    processed_data.append({\n",
    "        'text': generated_text,\n",
    "        'value': new_val\n",
    "    })\n",
    "\n",
    "# output_filename = f'train_{ratio:.2f}.json'\n",
    "output_filename = 'dataset/0.25_complete_train.json'\n",
    "\n",
    "# 保存新生成的数据\n",
    "with open(output_filename, 'w', encoding='utf-8') as file:\n",
    "    json.dump(processed_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"build data success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8013236-1cd2-4102-a960-10cd5dbf1a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
